{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd55b339-b438-4535-a271-1b208ff78d40",
   "metadata": {},
   "source": [
    "# Team 1 - Final Project\n",
    "## Jose Sandoval, Dheemanth Rajakumar, and Israel Romero Olvera\n",
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6425e2-0c9d-47d6-8be2-83482456987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848fbf4e-9397-4e52-8943-c38ce60a8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the directories to load the Midi files\n",
    "rootdir=r'C:\\Users\\isral\\AAI_511\\Composer_Dataset\\Composer_Dataset\\NN_midi_files_extended'\n",
    "traindir = os.path.join(rootdir,'train') #Training set\n",
    "testdir = os.path.join(rootdir,'test') #Testing set\n",
    "valdir = os.path.join(rootdir,'dev') #Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "699ee28c-9d1e-40b4-b6e0-4e1245dea5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a class to handle the MIDI loading\n",
    "class MidiDataset:\n",
    "    def __init__(self, train_dir, test_dir, seq_len=128, fs=10):\n",
    "        \"\"\"\n",
    "        train_dir: root folder with subfolders for each composer\n",
    "        seq_len: number of time steps per piano roll\n",
    "        fs: frames per second for piano roll\n",
    "        \"\"\"\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.val_dir = \"\"\n",
    "        self.seq_len = seq_len\n",
    "        self.fs = fs\n",
    "        self.train_samples = []\n",
    "        self.train_labels = []\n",
    "        self.train_label_to_idx = {}\n",
    "        self.test_samples = []\n",
    "        self.test_labels = []\n",
    "        self.test_label_to_idx = {}\n",
    "        self.val_samples = []\n",
    "        self.val_labels = []\n",
    "        self.val_label_to_idx = {}\n",
    "        self._load_data()\n",
    "    def _load_data(self):\n",
    "        self.train_labels, self.train_label_to_idx, self.train_samples = self.load_path(self.train_dir)\n",
    "        self.test_labels, self.test_label_to_idx, self.test_samples = self.load_path(self.test_dir)\n",
    "    def load_path(self, path):\n",
    "        labels = []\n",
    "        label_to_idx = {}\n",
    "        samples = []\n",
    "        #First, we load all the subfolders list, which represents the authors list\n",
    "        composer_dirs = [\n",
    "            directory for directory in os.listdir(self.train_dir)\n",
    "            if os.path.isdir(os.path.join(self.train_dir, directory))\n",
    "        ]\n",
    "        #Next, we load each individual midi file\n",
    "        for idx, composer in enumerate(composer_dirs):\n",
    "            label_to_idx[composer] = idx #Here we create a unique ID for each composer\n",
    "            #Now, let's load all individual files in each composer's folder\n",
    "            composer_path = os.path.join(path, composer)\n",
    "            for fname in os.listdir(composer_path):\n",
    "                if fname.endswith('.mid') or fname.endswith('.midi'): #Including any .MID or .MIDI files\n",
    "                    file_path = os.path.join(composer_path, fname)\n",
    "                    samples.append(file_path)\n",
    "                    labels.append(idx)\n",
    "        return labels, label_to_idx, samples\n",
    "    #This function will convert the midi notes into a piano roll. Only pitch and duration of notes will be preserved, but other attributes like note velocity or midi channel will be lost.\n",
    "    def midi_to_pianoroll(self, midi_path):\n",
    "        #Loading the midi file into memory\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "        #Converting the midi data into a piano roll. By default, the velocity values are preserved\n",
    "        piano_roll = midi_data.get_piano_roll(fs=self.fs)\n",
    "        #Make sure piano roll is binary: note active (1) or inactive (0), losing the velocity attribute: we just care if the note is on or off.\n",
    "        piano_roll = (piano_roll > 0).astype(np.float32)\n",
    "        # Transpose to (time, pitch), as TensorFlow will use the first dimension to establish sequenciality\n",
    "        piano_roll = piano_roll.T\n",
    "\n",
    "        # Pad or truncate to seq_len, to ensure all tensors are the same length (part of the melody might be lost, but the sample of the song should be good enough for classification.\n",
    "        if piano_roll.shape[0] < self.seq_len:\n",
    "            pad_width = self.seq_len - piano_roll.shape[0]\n",
    "            piano_roll = np.pad(piano_roll, ((0, pad_width), (0, 0)))\n",
    "        else:\n",
    "            piano_roll = piano_roll[:self.seq_len]\n",
    "\n",
    "        # Keep only 0-127 notes, losing other events like tempo changes, instrument changes, pitch bend, etc.\n",
    "        # NOTE: while foot switch messages get excluded too, the note duration is preserved in the piano roll as the note state is captured as \"ON\" by PrettyMIDI.\n",
    "        piano_roll = piano_roll[:, :128]\n",
    "\n",
    "        return piano_roll\n",
    "\n",
    "    def generator(self, samples, labels):\n",
    "        for midi_path, label in zip(self.train_samples, self.train_labels):\n",
    "            piano_roll = self.midi_to_pianoroll(midi_path)\n",
    "            yield piano_roll, label\n",
    "    #This function builds the TensorFlow dataset\n",
    "    def get_tf_dataset(self, samples, labels, batch_size=16, shuffle=True):\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape=(self.seq_len, 128), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: self.generator(samples, labels),\n",
    "            output_signature=output_signature\n",
    "        )\n",
    "        #The shuffle parameter will help the Neural Network training by shuffling the sample's order, preventing the model from seeing samples in the same order every epoch\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(self.train_samples))\n",
    "\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset\n",
    "    def get_tf_dataset_train(self, batch_size=16, shuffle=True):\n",
    "        return self.get_tf_dataset(self.train_samples, self.train_labels, batch_size, shuffle)\n",
    "    def get_tf_dataset_test(self, batch_size=16, shuffle=True):\n",
    "        return self.get_tf_dataset(self.test_samples, self.test_labels, batch_size, shuffle)\n",
    "    def load_validation_data(self, val_dir, batch_size=16, shuffle=True):\n",
    "        self.val_dir = val_dir\n",
    "        self.val_labels, self.val_label_to_idx, self.val_samples = self.load_path(val_dir)\n",
    "        return self.get_tf_dataset(self.val_samples, self.val_labels, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47c6928a-21e1-47c8-9a9f-a1f0d4c42590",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_builder = MidiDataset(train_dir=traindir, test_dir=testdir, seq_len=128, fs=10)\n",
    "train_dataset = dataset_builder.get_tf_dataset_train(batch_size=16)\n",
    "test_dataset = dataset_builder.get_tf_dataset_test(batch_size=16)\n",
    "\n",
    "val_dataset = dataset_builder.load_validation_data(val_dir=valdir, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9a2dbec-81fb-4c3c-8434-5db1dbdb55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch piano_rolls shape: (16, 128, 128)\n",
      "Batch labels: tf.Tensor([3 2 3 6 6 1 6 7 3 1 4 4 0 6 8 8], shape=(16,), dtype=int32)\n",
      "Labels batch shape: (16,)\n",
      "Batch piano_rolls shape: (16, 128, 128)\n",
      "Batch labels: tf.Tensor([5 5 8 4 4 2 5 0 2 7 3 8 6 6 0 2], shape=(16,), dtype=int32)\n",
      "Labels batch shape: (16,)\n",
      "Batch piano_rolls shape: (16, 128, 128)\n",
      "Batch labels: tf.Tensor([3 7 8 8 3 0 7 3 4 0 8 4 3 5 6 4], shape=(16,), dtype=int32)\n",
      "Labels batch shape: (16,)\n",
      "Batch piano_rolls shape: (16, 128, 128)\n",
      "Batch labels: tf.Tensor([4 2 3 4 6 4 4 0 2 1 6 2 7 3 7 2], shape=(16,), dtype=int32)\n",
      "Labels batch shape: (16,)\n",
      "Batch piano_rolls shape: (16, 128, 128)\n",
      "Batch labels: tf.Tensor([6 3 1 5 1 1 5 4 1 0 5 0 5 8 7 1], shape=(16,), dtype=int32)\n",
      "Labels batch shape: (16,)\n",
      "Batch piano_rolls shape: (16, 128, 128)\n",
      "Batch labels: tf.Tensor([2 7 3 6 3 6 6 1 8 3 3 5 7 2 5 4], shape=(16,), dtype=int32)\n",
      "Labels batch shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "for piano_rolls, labels in train_dataset.take(2):\n",
    "    print('Batch piano_rolls shape:', piano_rolls.shape)  # (batch_size, seq_len, 128)\n",
    "    print('Batch labels:', labels)                        # (batch_size,)\n",
    "    print('Labels batch shape:', labels.shape)\n",
    "\n",
    "for piano_rolls, labels in test_dataset.take(2):\n",
    "    print('Batch piano_rolls shape:', piano_rolls.shape)  # (batch_size, seq_len, 128)\n",
    "    print('Batch labels:', labels)                        # (batch_size,)\n",
    "    print('Labels batch shape:', labels.shape)\n",
    "for piano_rolls, labels in val_dataset.take(2):\n",
    "    print('Batch piano_rolls shape:', piano_rolls.shape)  # (batch_size, seq_len, 128)\n",
    "    print('Batch labels:', labels)                        # (batch_size,)\n",
    "    print('Labels batch shape:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "282a1ed2-2c5f-4be7-aba1-925f2f240508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.element_spec)\n",
    "print(test_dataset.element_spec)\n",
    "print(val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33a0c2c8-6b3d-4d83-b16b-efaa041151f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 369\n"
     ]
    }
   ],
   "source": [
    "sample_count = 0\n",
    "for piano_rolls, labels in val_dataset:\n",
    "    sample_count += piano_rolls.shape[0]  # Number of samples in this batch\n",
    "\n",
    "print(\"Total samples:\", sample_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
